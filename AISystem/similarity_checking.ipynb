{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated\n",
    "Not used because of low performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      family salient part day since day care child o...\n",
      "1      yoga keep focused able take time breath work b...\n",
      "2      yesterday family played bunch board game husba...\n",
      "3      yesterday visited parent dinner seen week wond...\n",
      "4      yesterday really felt importance health went b...\n",
      "                             ...                        \n",
      "995    last night spent night wife watching good movi...\n",
      "996    finished work preparing recipe dinner free tim...\n",
      "997    get bit exercise tody dog apple watch say quit...\n",
      "998    managed write two paper day two class proud ta...\n",
      "999    woke hour particular reason bad tired day need...\n",
      "Name: Answer, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "journal_entries = pd.read_csv('archive/data.csv')\n",
    "train = journal_entries['Answer'][:1000]\n",
    "test = journal_entries['Answer'][1000:]\n",
    "data = train.apply(preprocess)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarity score: 0.33912027270956036\n",
      "Maximum similarity score for false input: 0.22577622745582532\n"
     ]
    }
   ],
   "source": [
    "# Lexical analysis\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_similarity(input_text, data_vectors, vectorizer):\n",
    "    input_processed = preprocess(input_text)\n",
    "    input_vector = vectorizer.transform([input_processed])\n",
    "    similarities = cosine_similarity(input_vector, data_vectors).flatten()\n",
    "    return similarities\n",
    "\n",
    "# Example input text\n",
    "input_text = \"Today i take a walk with my dog and i feel so happy. I love my dog so much. I made a few mistakes today but i will learn from them. I am grateful for my family and friends. I work at as a dropshipper today and it feels pretty fun honestly. Hoping to see a better day tomorrow.\"\n",
    "input_false = \"I would like to say thanks to all Real Madrid fans for having me in the last 3 years\"\n",
    "\n",
    "# Calculate similarity to the journal entries\n",
    "similarities = calculate_similarity(input_text, X, vectorizer)\n",
    "\n",
    "# Get the highest similarity score\n",
    "max_similarity = max(similarities)\n",
    "max_similarity_false = max(calculate_similarity(input_false, X, vectorizer))\n",
    "\n",
    "print(f\"Maximum similarity score: {max_similarity}\")\n",
    "print(f\"Maximum similarity score for false input: {max_similarity_false}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity score for test: 0.32824361146188463\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "similarity_scores = []\n",
    "for t in test:\n",
    "    similarities = calculate_similarity(t, X, vectorizer)\n",
    "    max_similarity = max(similarities)\n",
    "    similarity_scores.append(max_similarity)\n",
    "    # print(f\"Maximum similarity score for test: {max_similarity}\")\n",
    "    \n",
    "print(f\"Average similarity score for test: {np.mean(similarity_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
